# ğŸš€ Intermediate RAG System

> A production-oriented Retrieval-Augmented Generation (RAG) pipeline built with open-source tools. This system prioritizes correctness, debuggability, and scalability over quick demos.

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-active-success.svg)]()

---

## ğŸ“– Overview

This project implements a complete RAG system that:

- âœ… **Ingests and processes** documents (TXT, PDF)
- âœ… **Generates semantic embeddings** using sentence transformers
- âœ… **Performs vector similarity search** with ChromaDB
- âœ… **Generates grounded answers** using local LLMs via Ollama
- âœ… **Maintains metadata and logs** in a traditional database

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Embedding  â”‚ â† Sentence Transformers (384d)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB Vector Search  â”‚ â† Cosine similarity search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top-K Relevant Chunks  â”‚ â† Retrieve 3-5 most relevant
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Formatting   â”‚ â† Build prompt with context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local LLM (Ollama)   â”‚ â† Generate grounded answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grounded Answer + Sources   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
rag_project/
â”‚
â”œâ”€â”€ rag/                          # Core RAG pipeline logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loaders/                  # Document ingestion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ loader.py             # PDF, TXT, web loaders
â”‚   â”‚
â”‚   â”œâ”€â”€ chunking/                 # Text splitting strategies
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ recursive.py          # Recursive character splitting
â”‚   â”‚   â””â”€â”€ semantic.py           # Semantic-based chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/               # Vector generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ hf_embeddings.py      # HuggingFace sentence transformers
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/              # Vector database management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chroma_store.py       # ChromaDB integration
â”‚   â”‚
â”‚   â”œâ”€â”€ retriever/                # Search and ranking
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py          # Similarity search
â”‚   â”‚   â””â”€â”€ reranker.py           # Result reranking
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                  # LLM prompt templates
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ templates.py          # System/user prompts
â”‚   â”‚
â”‚   â””â”€â”€ llm/                      # LLM integration
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ollama_llm.py         # Local Ollama client
â”‚
â”œâ”€â”€ db/                           # Metadata & logging
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ models.py                 # SQLite/PostgreSQL schemas
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Source documents (gitignored)
â”‚   â”œâ”€â”€ processed/                # Cleaned text chunks
â”‚   â””â”€â”€ chroma/                   # Vector database storage
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter experiments
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_chunking_strategies.ipynb
â”‚   â””â”€â”€ 03_retrieval_evaluation.ipynb
â”‚
â”œâ”€â”€ scripts/                      # CLI tools
â”‚   â”œâ”€â”€ ingest.py                 # Batch document ingestion
â”‚   â”œâ”€â”€ build_index.py            # Build vector index
â”‚   â””â”€â”€ query.py                  # Interactive query tool
â”‚
â”œâ”€â”€ tests/                        # Unit & integration tests
â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ config.yaml                   # Configuration file
â”œâ”€â”€ pyproject.toml                # Project metadata
â”œâ”€â”€ requirements.txt              # Pinned dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Technology Stack

| Component          | Technology                                    | Purpose                           |
|--------------------|-----------------------------------------------|-----------------------------------|
| **Language**       | Python 3.12+                                  | Core programming language         |
| **Framework**      | LangChain                                     | RAG orchestration                 |
| **Vector Store**   | ChromaDB                                      | Embedding storage & search        |
| **Embeddings**     | Sentence Transformers (all-MiniLM-L6-v2)      | Text â†’ 384d vectors               |
| **LLM**            | Ollama (llama2, mistral, etc.)                | Local inference                   |
| **Database**       | SQLite / PostgreSQL                           | Metadata & logs                   |
| **Package Mgr**    | uv / pip                                      | Dependency management             |

---

## ğŸ§  Embedding Model: all-MiniLM-L6-v2

We use `sentence-transformers/all-MiniLM-L6-v2` which produces **384-dimensional embeddings**:

- âœ… **384 dimensions** capture semantic meaning
- âœ… Each dimension represents a fragment of context
- âœ… Similar meanings â†’ vectors close in space
- âœ… No single dimension is interpretable
- âœ… **Meaning emerges from all 384 dimensions combined**

### Why This Model?

| Criteria      | Rating | Notes                                  |
|---------------|--------|----------------------------------------|
| Speed         | â­â­â­â­â­ | Fast inference (~5ms per sentence)     |
| Size          | â­â­â­â­â­ | Compact vectors (384d vs 768d/1024d)   |
| Quality       | â­â­â­â­   | Good semantic understanding            |
| Memory        | â­â­â­â­â­ | Low RAM usage                          |

---

## ğŸ“Œ Core RAG Components (One-Line Explanations)

### 1. **rag/**
Root module containing the full Retrieval-Augmented Generation pipeline.

### 2. **rag/loaders/**
Loads raw data from files or sources and converts it into clean text.
- `loader.py` â€” Handles ingestion of PDFs, text, web pages, or datasets.

### 3. **rag/chunking/**
Splits large documents into smaller, meaningful text chunks.
- `recursive.py` â€” Recursively splits text by structure while preserving context.
- `semantic.py` â€” Splits text based on semantic meaning rather than fixed size.

### 4. **rag/embeddings/**
Converts text chunks into numerical vector embeddings.
- `hf_embeddings.py` â€” Generates embeddings using HuggingFace models.

### 5. **rag/vectorstore/**
Stores and retrieves embeddings using a vector database.
- `chroma_store.py` â€” Manages embedding storage and similarity search via ChromaDB.

### 6. **rag/retriever/**
Fetches the most relevant chunks for a given user query.
- `retriever.py` â€” Performs vector similarity search.
- `reranker.py` â€” Reorders retrieved chunks for higher relevance and accuracy.

### 7. **rag/prompts/**
Contains prompt templates that guide how the LLM uses retrieved context.
- `templates.py` â€” System and user prompt templates.

### 8. **rag/llm/**
Handles interaction with the language model for final answer generation.
- `ollama_llm.py` â€” Sends context and queries to a local Ollama-hosted LLM.

### 9. **__init__.py (all folders)**
Marks directories as Python modules and enables clean imports.

---

## ğŸ¯ Design Philosophy

| Principle                  | Description                                                          |
|----------------------------|----------------------------------------------------------------------|
| **Retrieval Quality First** | The quality of retrieved context matters more than LLM sophistication |
| **Honest Uncertainty**      | System rejects queries when relevant knowledge is missing            |
| **No Hallucinations**       | Answers must be grounded in retrieved documents                      |
| **Progressive Complexity**  | Simple â†’ Correct â†’ Scalable                                          |

> **Key Principle**: RAG is **80% retrieval and data quality**, **20% generation**.

---

## ğŸš€ Getting Started

### Prerequisites

- âœ… Python 3.12+
- âœ… [Ollama](https://ollama.ai/) installed and running
- âœ… `uv` package manager (optional but recommended)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd rag_project

# Install dependencies
pip install -r requirements.txt

# Or using uv (faster)
uv pip install -r requirements.txt

# Pull an LLM model (if using Ollama)
ollama pull llama2
```

### Quick Start

```bash
# 1. Place your documents in data/raw/
# Example: data/raw/company_docs.pdf

# 2. Process documents
python scripts/ingest.py

# 3. Build vector index
python scripts/build_index.py

# 4. Run queries
python scripts/query.py "What is the main topic discussed in the documents?"
```

---

## ğŸ’» Usage Examples

### Basic Query

```python
from rag import RAGPipeline

# Initialize the pipeline
pipeline = RAGPipeline()

# Query the system
result = pipeline.query("What is the main topic discussed in the documents?")

print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
print(f"Confidence: {result['confidence']}")
```

### Advanced Configuration

```python
from rag import RAGPipeline

pipeline = RAGPipeline(
    chunk_size=512,
    chunk_overlap=50,
    top_k=5,
    model_name="llama2",
    temperature=0.7
)

result = pipeline.query(
    query="Explain the product roadmap",
    temperature=0.7,
    max_tokens=500,
    return_sources=True
)
```

### Batch Processing

```python
from rag import RAGPipeline

pipeline = RAGPipeline()

queries = [
    "What are the key features?",
    "Who are the competitors?",
    "What is the pricing model?"
]

results = pipeline.batch_query(queries)

for query, result in zip(queries, results):
    print(f"Q: {query}")
    print(f"A: {result['answer']}\n")
```

---

## ğŸ“Š Development Status

| Feature                     | Status           | Priority |
|-----------------------------|------------------|----------|
| Document ingestion          | âœ… Done          | High     |
| Text chunking               | âœ… Done          | High     |
| Embedding generation        | âœ… Done          | High     |
| Vector storage (ChromaDB)   | âœ… Done          | High     |
| Similarity search           | ğŸš§ In Progress   | High     |
| LLM integration             | ğŸš§ In Progress   | High     |
| Reranking                   | ğŸš§ In Progress   | Medium   |
| Evaluation framework        | ğŸ“‹ Planned       | Medium   |
| Query optimization          | ğŸ“‹ Planned       | Low      |
| Web UI (Streamlit)          | ğŸ“‹ Planned       | Low      |

**Legend**: âœ… Done | ğŸš§ In Progress | ğŸ“‹ Planned

---

## ğŸ—ºï¸ Roadmap

### Phase 1: Core Pipeline âœ…
- [x] Document ingestion (PDF, TXT)
- [x] Vector embeddings (all-MiniLM-L6-v2)
- [x] ChromaDB integration
- [ ] Similarity search (90% complete)

### Phase 2: LLM Integration ğŸš§
- [ ] Ollama integration
- [ ] Prompt engineering
- [ ] Answer generation
- [ ] Citation tracking

### Phase 3: Enhancement ğŸ“‹
- [ ] Reranking layer (cross-encoder)
- [ ] Hybrid search (vector + keyword)
- [ ] Query expansion
- [ ] Multi-query retrieval

### Phase 4: Production ğŸ“‹
- [ ] Evaluation metrics (precision, recall)
- [ ] Performance monitoring
- [ ] API deployment (FastAPI)
- [ ] Streamlit web UI
- [ ] Docker containerization

---

## âš™ï¸ Configuration

Edit `config.yaml` or set environment variables:

```yaml
# Vector Store
chroma:
  persist_directory: "./data/chroma"
  collection_name: "documents"
  distance_metric: "cosine"  # cosine, l2, ip

# Embeddings
embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"  # cpu, cuda, mps
  batch_size: 32

# Chunking
chunking:
  chunk_size: 512
  chunk_overlap: 50
  strategy: "recursive"  # recursive, semantic

# Retrieval
retrieval:
  top_k: 5
  score_threshold: 0.7
  rerank: false

# LLM
llm:
  provider: "ollama"
  model: "llama2"  # llama2, mistral, codellama
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9

# Logging
logging:
  level: "INFO"
  file: "logs/rag.log"
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=rag tests/

# Run specific test module
pytest tests/test_retrieval.py -v

# Run with verbose output
pytest -vv

# Generate HTML coverage report
pytest --cov=rag --cov-report=html tests/
```

---

## ğŸ“Š Vector Database Comparison

Use this table to choose a Vector DB based on your actual needs, not hype.

| Vector DB        | Speed | Cost | Scale | Simplicity | Metadata | Cloud/Local | Stage              | Best Use Case                          |
|------------------|-------|------|-------|------------|----------|-------------|--------------------|----------------------------------------|
| **FAISS**        | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ | âŒ | Local | Learning/Research | Maximum speed, custom systems, research |
| **ChromaDB**     | â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | âœ… | Local | Learning/Prototyping | RAG pipelines, local apps, fast iteration |
| **Qdrant**       | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | âœ… | Both | Learningâ†’Production | Strong filtering, self-hosted or cloud |
| **Weaviate**     | â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | âœ… | Both | Production | Hybrid search, schema-based retrieval |
| **Milvus**       | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­ | âœ… | Both | Production (Large) | Billions of vectors, distributed systems |
| **Pinecone**     | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­ | âœ… | Cloud | Production | Managed service, zero ops |
| **Elasticsearch**| â­â­â­ | â­â­ | â­â­â­â­ | â­â­ | âœ… | Both | Production | Keyword + vector hybrid search |
| **OpenSearch**   | â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­ | âœ… | Both | Production | Open-source ES alternative |

### ğŸ§  How to Read This Table

| Dimension    | Explanation                                                                 |
|--------------|-----------------------------------------------------------------------------|
| **Speed**    | Raw similarity search performance (FAISS & Milvus are fastest)              |
| **Cost**     | â­â­â­â­â­ = free/local, â­â­ = paid/managed infrastructure                          |
| **Scale**    | How many vectors you can handle (ChromaDB â†’ millions, Milvus â†’ billions)    |
| **Simplicity**| How fast you can get started with minimal infrastructure                   |
| **Metadata** | Ability to store & filter by document info (critical for real RAG systems)  |

### ğŸ’¡ Our Choice: ChromaDB

We chose **ChromaDB** for this project because:

- âœ… **Zero ops** - No server setup required
- âœ… **Local first** - Perfect for learning & prototyping
- âœ… **Metadata support** - Filter by document properties
- âœ… **Fast enough** - Handles millions of vectors
- âœ… **Easy to upgrade** - Can migrate to Qdrant/Weaviate later

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run linting
ruff check .

# Format code
black .

# Type checking
mypy rag/
```

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **LangChain** - RAG framework and orchestration
- **ChromaDB** - Vector storage and similarity search
- **Sentence Transformers** - High-quality embeddings
- **Ollama** - Local LLM inference
- **HuggingFace** - Open-source models

---

## ğŸ“§ Contact

For questions, feedback, or collaboration:

- ğŸ“§ Email: your- jyotiradityaparihar@gmail.com
- ğŸ’¬ GitHub Issues: [Open an issue](https://github.com/your-repo/issues)
- [in] Linkdin : https://www.linkedin.com/in/jyotiraditya-singh-959488248/

---

## ğŸ“š Additional Resources

- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Ollama Models](https://ollama.ai/library)
- [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)

---

<div align="center">

**Built with â¤ï¸ for production-grade RAG systems**

â­ **Star this repo** if you find it helpful!

</div>