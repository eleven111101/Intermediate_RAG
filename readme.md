# Intermediate RAG System

A production-oriented Retrieval-Augmented Generation (RAG) pipeline built with open-source tools. This system prioritizes correctness, debuggability, and scalability over quick demos.

## Overview

This project implements a complete RAG system that:
- Ingests and processes documents (TXT, PDF)
- Generates semantic embeddings using sentence transformers
- Performs vector similarity search with ChromaDB
- Generates grounded answers using local LLMs via Ollama
- Maintains metadata and operational logs in a traditional database

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Embedding  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB Vector Search  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top-K Relevant Chunksâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Formatting â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local LLM (Ollama)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grounded Answer + Sourcesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
rag_project/
â”œâ”€â”€ rag/                    # Core RAG pipeline logic
â”œâ”€â”€ db/                     # Database layer (metadata, logs)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Source documents (gitignored)
â”‚   â””â”€â”€ processed/         # Cleaned text for indexing
â”œâ”€â”€ notebooks/             # Jupyter notebooks for experiments
â”œâ”€â”€ scripts/               # CLI tools and batch processing
â”œâ”€â”€ pyproject.toml         # Project metadata and dependencies
â”œâ”€â”€ requirements.txt       # Pinned dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Technology Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.12 |
| **Framework** | LangChain |
| **Vector Store** | ChromaDB |
| **Embeddings** | Sentence Transformers (all-MiniLM-L6-v2) |
| **LLM** | Ollama (local inference) |
| **Database** | SQLite / PostgreSQL |
| **Package Manager** | uv |

## Embedding Model: all-MiniLM-L6-v2

We use `sentence-transformers/all-MiniLM-L6-v2` which produces **384-dimensional embeddings**:

- Each text chunk is converted to a 384-dimensional vector
- Each dimension captures a fragment of semantic meaning
- Similar meanings produce vectors that are close in vector space
- No single dimension is interpretableâ€”**meaning emerges from all 384 dimensions combined**

This model balances:
- âœ… Speed (fast inference)
- âœ… Size (compact vectors)
- âœ… Quality (good semantic understanding)

## Design Philosophy

1. **Retrieval Quality First**: The quality of retrieved context matters more than LLM sophistication
2. **Honest Uncertainty**: System rejects queries when relevant knowledge is missing
3. **No Hallucinations**: Answers must be grounded in retrieved documents
4. **Progressive Complexity**: Simple â†’ Correct â†’ Scalable

> **Key Principle**: RAG is 80% retrieval and data quality, 20% generation.

## Getting Started

### Prerequisites

- Python 3.12+
- [Ollama](https://ollama.ai/) installed and running
- `uv` package manager (optional but recommended)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd rag_project

# Install dependencies
pip install -r requirements.txt

# Or using uv
uv pip install -r requirements.txt
```

### Quick Start

```bash
# 1. Place your documents in data/raw/

# 2. Process documents
python scripts/ingest.py

# 3. Build vector index
python scripts/build_index.py

# 4. Run queries
python scripts/query.py "Your question here"
```

## Usage Examples

### Basic Query

```python
from rag import RAGPipeline

# Initialize the pipeline
pipeline = RAGPipeline()

# Query the system
result = pipeline.query("What is the main topic discussed in the documents?")

print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
```

### Advanced Configuration

```python
from rag import RAGPipeline

pipeline = RAGPipeline(
    chunk_size=512,
    chunk_overlap=50,
    top_k=5,
    model_name="llama2"
)

result = pipeline.query(
    query="Your question",
    temperature=0.7,
    max_tokens=500
)
```

## Development Status

| Feature | Status |
|---------|--------|
| Document ingestion | ğŸš§ In Progress |
| Text chunking | ğŸ“‹ Planned |
| Embedding generation | ğŸ“‹ Planned |
| Vector storage (ChromaDB) | ğŸ“‹ Planned |
| Similarity search | ğŸ“‹ Planned |
| LLM integration | ğŸ“‹ Planned |
| Reranking | ğŸ“‹ Planned |
| Evaluation framework | ğŸ“‹ Planned |
| Query optimization | ğŸ“‹ Planned |

## Roadmap

### Phase 1: Core Pipeline âœ…
- [ ] Document ingestion
- [ ] Vector embeddings
- [ ] Similarity search

### Phase 2: LLM Integration ğŸš§
- [ ] Ollama integration
- [ ] Prompt engineering
- [ ] Answer generation

### Phase 3: Enhancement ğŸ“‹
- [ ] Reranking layer
- [ ] Hybrid search (vector + keyword)
- [ ] Query expansion

### Phase 4: Production ğŸ“‹
- [ ] Evaluation metrics
- [ ] Performance monitoring
- [ ] API deployment

## Configuration

Edit `config.yaml` or set environment variables:

```yaml
# Vector Store
chroma:
  persist_directory: "./data/chroma"
  collection_name: "documents"

# Embeddings
embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"

# Chunking
chunking:
  chunk_size: 512
  chunk_overlap: 50

# LLM
llm:
  provider: "ollama"
  model: "llama2"
  temperature: 0.7
  max_tokens: 500
```

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=rag tests/

# Run specific test module
pytest tests/test_retrieval.py
```

## Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- LangChain for the RAG framework
- ChromaDB for vector storage
- Sentence Transformers for embeddings
- Ollama for local LLM inference

## Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for production-grade RAG systems**