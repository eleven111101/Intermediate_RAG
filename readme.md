# üöÄ Intermediate RAG System

> A production-oriented Retrieval-Augmented Generation (RAG) pipeline built with open-source tools. This system prioritizes correctness, debuggability, and scalability over quick demos.

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-active-success.svg)]()

---

## üìñ Overview

This project implements a complete RAG system that:

- ‚úÖ **Ingests and processes** documents (TXT, PDF)
- ‚úÖ **Generates semantic embeddings** using sentence transformers
- ‚úÖ **Performs vector similarity search** with ChromaDB
- ‚úÖ **Generates grounded answers** using local LLMs via Ollama
- ‚úÖ **Maintains metadata and logs** in a traditional database
- ‚úÖ **Automatic pipeline orchestration** with intelligent system checks

---

## üîß Current Capabilities

| Feature | Status | Notes |
|---------|--------|-------|
| Automatic ingestion pipeline | ‚úÖ Complete | Single entry-point execution |
| Recursive + semantic chunking | ‚úÖ Complete | Dual-strategy approach |
| Sentence-Transformer embeddings | ‚úÖ Complete | 384-dimensional vectors |
| Persistent ChromaDB vector store | ‚úÖ Complete | HNSW + cosine similarity |
| Query-time vector retrieval | ‚úÖ Complete | Top-K similarity search |
| Optional reranking layer | üöß In Progress | Placeholder implemented |
| LLM answer generation | üìã Planned | Next major milestone |

**Legend**: ‚úÖ Complete | üöß In Progress | üìã Planned

---

## üèóÔ∏è Architecture

### System Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Query     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Query Embedding  ‚îÇ ‚Üê Sentence Transformers (384d)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ChromaDB Vector Search  ‚îÇ ‚Üê Cosine similarity search
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Top-K Relevant Chunks  ‚îÇ ‚Üê Retrieve 3-5 most relevant
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Context Formatting   ‚îÇ ‚Üê Build prompt with context
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Local LLM (Ollama)   ‚îÇ ‚Üê Generate grounded answer
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Grounded Answer + Sources   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### High-Level Architecture

```
main.py
  ‚îî‚îÄ‚îÄ checker_files.py
        ‚îú‚îÄ‚îÄ checks raw data
        ‚îú‚îÄ‚îÄ checks vector DB
        ‚îî‚îÄ‚îÄ decides: INGEST or QUERY

INGESTION FLOW
  raw data
    ‚Üí loaders
    ‚Üí recursive chunking
    ‚Üí semantic chunking
    ‚Üí embeddings
    ‚Üí ChromaDB (persistent)

QUERY FLOW
  user query
    ‚Üí embeddings
    ‚Üí retriever (vector search)
    ‚Üí reranker (optional)
    ‚Üí context output
    ‚Üí LLM generation (planned)
```

---

## üìÅ Project Structure

```
INTERMEDIATE_RAG/
‚îÇ
‚îú‚îÄ‚îÄ main.py                       # Single entry point - automatic mode detection
‚îú‚îÄ‚îÄ config.yaml                   # System configuration
‚îú‚îÄ‚îÄ requirements.txt              # Pinned dependencies
‚îú‚îÄ‚îÄ pyproject.toml               # Project metadata
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ rag_project/
‚îÇ   ‚îú‚îÄ‚îÄ checker_files.py          # System validator & decision engine
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py          # Ingestion pipeline orchestrator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query_rag.py          # Query pipeline orchestrator
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ rag/                      # Core RAG pipeline logic
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ loaders/              # Document ingestion
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ loader.py         # PDF, TXT, web loaders
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ chunking/             # Text splitting strategies
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ recursive.py      # Recursive character splitting
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ semantic.py       # Semantic-based chunking
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ embeddings/           # Vector generation
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ hf_embeddings.py  # HuggingFace sentence transformers
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ chromaDB/             # Vector database management
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ chroma_store.py   # ChromaDB integration
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ retriever/            # Search and ranking
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py      # Similarity search
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ reranker.py       # Result reranking (placeholder)
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ prompts/              # LLM prompt templates
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ templates.py      # System/user prompts
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ llm/                  # LLM integration
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îî‚îÄ‚îÄ ollama_llm.py     # Local Ollama client (planned)
‚îÇ
‚îú‚îÄ‚îÄ db/                           # Metadata & logging
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ models.py                 # SQLite/PostgreSQL schemas
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Source documents (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Cleaned text chunks
‚îÇ   ‚îî‚îÄ‚îÄ chroma/                   # Vector database storage (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ vector_store/
‚îÇ   ‚îî‚îÄ‚îÄ chroma/                   # Persistent ChromaDB storage
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                    # Jupyter experiments
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_chunking_strategies.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_retrieval_evaluation.ipynb
‚îÇ
‚îî‚îÄ‚îÄ tests/                        # Unit & integration tests
    ‚îú‚îÄ‚îÄ test_chunking.py
    ‚îú‚îÄ‚îÄ test_retrieval.py
    ‚îî‚îÄ‚îÄ test_pipeline.py
```

---

## üõ†Ô∏è Technology Stack

| Component          | Technology                                    | Purpose                           |
|--------------------|-----------------------------------------------|-----------------------------------|
| **Language**       | Python 3.12+                                  | Core programming language         |
| **Framework**      | LangChain                                     | RAG orchestration                 |
| **Vector Store**   | ChromaDB                                      | Embedding storage & search        |
| **Embeddings**     | Sentence Transformers (all-MiniLM-L6-v2)      | Text ‚Üí 384d vectors               |
| **LLM**            | Ollama (llama2, mistral, etc.)                | Local inference (planned)         |
| **Database**       | SQLite / PostgreSQL                           | Metadata & logs                   |
| **Package Mgr**    | uv / pip                                      | Dependency management             |

---

## üß† Understanding the Components

### What is RAG?

Retrieval-Augmented Generation (RAG) combines:

- **Information Retrieval** (vector search)
- **Language Models** (LLMs)

Instead of relying only on model memory, the LLM is given relevant retrieved context at query time.

```
User Query
  ‚Üí Embed query
  ‚Üí Retrieve relevant chunks
  ‚Üí Provide context to LLM
  ‚Üí Generate grounded answer
```

### Embedding Model: all-MiniLM-L6-v2

We use `sentence-transformers/all-MiniLM-L6-v2` which produces **384-dimensional embeddings**:

- ‚úÖ **384 dimensions** capture semantic meaning
- ‚úÖ Each dimension represents a fragment of context
- ‚úÖ Similar meanings ‚Üí vectors close in space
- ‚úÖ No single dimension is interpretable
- ‚úÖ **Meaning emerges from all 384 dimensions combined**

#### Why This Model?

| Criteria      | Rating | Notes                                  |
|---------------|--------|----------------------------------------|
| Speed         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Fast inference (~5ms per sentence)     |
| Size          | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Compact vectors (384d vs 768d/1024d)   |
| Quality       | ‚≠ê‚≠ê‚≠ê‚≠ê   | Good semantic understanding            |
| Memory        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Low RAM usage                          |

---

## üìå Core RAG Components

### 1. **rag/**
Root module containing the full Retrieval-Augmented Generation pipeline.

### 2. **rag/loaders/**
Loads raw data from files or sources and converts it into clean text.
- `loader.py` ‚Äî Handles ingestion of PDFs, text, web pages, or datasets.

### 3. **rag/chunking/**
Splits large documents into smaller, meaningful text chunks.
- `recursive.py` ‚Äî Recursively splits text by structure while preserving context.
- `semantic.py` ‚Äî Splits text based on semantic meaning rather than fixed size.

**Chunking Strategy:**
```
Recursive chunking ‚Üí preserves structure
Semantic chunking ‚Üí improves meaning coherence
Both applied sequentially (intermediate RAG level)
```

### 4. **rag/embeddings/**
Converts text chunks into numerical vector embeddings.
- `hf_embeddings.py` ‚Äî Generates embeddings using HuggingFace models.

### 5. **rag/chromaDB/**
Stores and retrieves embeddings using a vector database.
- `chroma_store.py` ‚Äî Manages embedding storage and similarity search via ChromaDB.
- Uses HNSW + cosine similarity for efficient retrieval

### 6. **rag/retriever/**
Fetches the most relevant chunks for a given user query.
- `retriever.py` ‚Äî Performs vector similarity search.
- `reranker.py` ‚Äî Reorders retrieved chunks for higher relevance and accuracy (placeholder).

### 7. **rag/prompts/**
Contains prompt templates that guide how the LLM uses retrieved context.
- `templates.py` ‚Äî System and user prompt templates.

### 8. **rag/llm/**
Handles interaction with the language model for final answer generation.
- `ollama_llm.py` ‚Äî Sends context and queries to a local Ollama-hosted LLM (planned).

### 9. **checker_files.py**
System validator and decision engine that:
- Checks if raw data exists
- Checks if vector DB exists
- Automatically decides: INGEST or QUERY mode
- No manual flags or mode switching required

---

## üéØ Design Philosophy

| Principle                  | Description                                                          |
|----------------------------|----------------------------------------------------------------------|
| **Retrieval Quality First** | The quality of retrieved context matters more than LLM sophistication |
| **Honest Uncertainty**      | System rejects queries when relevant knowledge is missing            |
| **No Hallucinations**       | Answers must be grounded in retrieved documents                      |
| **Progressive Complexity**  | Simple ‚Üí Correct ‚Üí Scalable                                          |
| **Clean Separation**        | Modular design for easy testing and deployment                       |
| **Automatic Orchestration** | Intelligent system checks eliminate manual configuration             |

> **Key Principle**: RAG is **80% retrieval and data quality**, **20% generation**.

---

## üöÄ Getting Started

### Prerequisites

- ‚úÖ Python 3.12+
- ‚úÖ [Ollama](https://ollama.ai/) installed and running (for LLM generation - planned)
- ‚úÖ `uv` package manager (optional but recommended)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd INTERMEDIATE_RAG

# Install dependencies
pip install -r requirements.txt

# Or using uv (faster)
uv pip install -r requirements.txt

# Pull an LLM model (if using Ollama - for future use)
ollama pull llama2
```

### Quick Start

```bash
# 1. Place your documents in data/raw/
# Example: data/raw/company_docs.pdf, data/raw/report.txt

# 2. Run the system (automatic mode detection)
python main.py

# System will automatically:
# - Detect if ingestion is needed
# - Process documents and build vector index
# - OR allow you to query if index exists
```

### How the System Runs

**Single Command:**
```bash
python main.py
```

**What Happens Automatically:**

1. **System Check**
   - Raw data exists?
   - ChromaDB exists?

2. **Decision**
   - ‚ùå No DB ‚Üí run ingestion
   - ‚úÖ DB exists ‚Üí run query

**No manual flags. No mode switching.**

---

## üì• Ingestion Pipeline

### What It Does

1. Load raw documents (.txt, .pdf)
2. Recursive chunking (structure-based)
3. Temporary embeddings
4. Semantic chunking (meaning-based)
5. Final embeddings
6. Store vectors in ChromaDB (persistent)

### Sample Output

```
==============================
 RAG INGESTION PIPELINE STARTED
==============================

[STEP 1] Loading raw documents
‚Üí Loaded documents: 1

[STEP 2] Recursive chunking
‚Üí Recursive chunks: 67

[STEP 3] Generating temporary embeddings
‚Üí Temporary embeddings generated

[STEP 4] Semantic chunking
‚Üí Semantic chunks: 128

[STEP 5] Generating final embeddings
‚Üí Final embeddings: 128

[STEP 6] Storing in ChromaDB
‚Üí Stored 128 chunks in ChromaDB

==============================
 INGESTION PIPELINE COMPLETED
==============================
```

---

## üîç Query Pipeline

### What It Does

1. Load persistent ChromaDB
2. Display DB metadata
3. Accept user query
4. Embed query
5. Retrieve top-K chunks
6. (Optional) rerank
7. Print retrieved context
8. (Planned) Generate answer with LLM

### Sample Output

```
==============================
 RAG QUERY SERVICE
==============================
Vector DB Path  : vector_store/chroma
Collection Name: documents
Top-K          : 5
==============================

Ask a question: What is The Ashwa Riders?

=== RETRIEVED CONTEXT ===

[1] THE ASHWA RIDERS
OFF-ROAD ATV BUSINESS OVERVIEW...
----

[2] Marketing Strategy
The Ashwa Riders focuses on...
----
```

---

## üíª Usage Examples

### Basic Query

```python
from rag import RAGPipeline

# Initialize the pipeline
pipeline = RAGPipeline()

# Query the system
result = pipeline.query("What is the main topic discussed in the documents?")

print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
print(f"Confidence: {result['confidence']}")
```

### Advanced Configuration

```python
from rag import RAGPipeline

pipeline = RAGPipeline(
    chunk_size=512,
    chunk_overlap=50,
    top_k=5,
    model_name="llama2",
    temperature=0.7
)

result = pipeline.query(
    query="Explain the product roadmap",
    temperature=0.7,
    max_tokens=500,
    return_sources=True
)
```

### Batch Processing

```python
from rag import RAGPipeline

pipeline = RAGPipeline()

queries = [
    "What are the key features?",
    "Who are the competitors?",
    "What is the pricing model?"
]

results = pipeline.batch_query(queries)

for query, result in zip(queries, results):
    print(f"Q: {query}")
    print(f"A: {result['answer']}\n")
```

---

## ‚öôÔ∏è Configuration

Edit `config.yaml` to customize system behavior:

```yaml
# Vector Store
chroma:
  persist_directory: "./vector_store/chroma"
  collection_name: "documents"
  distance_metric: "cosine"  # cosine, l2, ip

# Embeddings
embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"  # cpu, cuda, mps
  batch_size: 32

# Chunking
chunking:
  # Recursive chunking
  chunk_size: 512
  chunk_overlap: 50
  strategy: "recursive"  # recursive, semantic
  
  # Semantic chunking
  similarity_threshold: 0.5
  min_chunk_size: 100

# Retrieval
retrieval:
  top_k: 5
  score_threshold: 0.7
  rerank: false
  rerank_top_n: 10

# LLM (Planned)
llm:
  provider: "ollama"
  model: "llama2"  # llama2, mistral, codellama
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9

# Logging
logging:
  level: "INFO"
  file: "logs/rag.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

---

## üìä Vector Database Comparison

Use this table to choose a Vector DB based on your actual needs, not hype.

| Vector DB        | Speed | Cost | Scale | Simplicity | Metadata | Cloud/Local | Stage              | Best Use Case                          |
|------------------|-------|------|-------|------------|----------|-------------|--------------------|----------------------------------------|
| **FAISS**        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ùå | Local | Learning/Research | Maximum speed, custom systems, research |
| **ChromaDB**     | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Local | Learning/Prototyping | RAG pipelines, local apps, fast iteration |
| **Qdrant**       | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Both | Learning‚ÜíProduction | Strong filtering, self-hosted or cloud |
| **Weaviate**     | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ | Both | Production | Hybrid search, schema-based retrieval |
| **Milvus**       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ | Both | Production (Large) | Billions of vectors, distributed systems |
| **Pinecone**     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Cloud | Production | Managed service, zero ops |
| **Elasticsearch**| ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ | Both | Production | Keyword + vector hybrid search |
| **OpenSearch**   | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ | Both | Production | Open-source ES alternative |

### üß† How to Read This Table

| Dimension    | Explanation                                                                 |
|--------------|-----------------------------------------------------------------------------|
| **Speed**    | Raw similarity search performance (FAISS & Milvus are fastest)              |
| **Cost**     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê = free/local, ‚≠ê‚≠ê = paid/managed infrastructure                          |
| **Scale**    | How many vectors you can handle (ChromaDB ‚Üí millions, Milvus ‚Üí billions)    |
| **Simplicity**| How fast you can get started with minimal infrastructure                   |
| **Metadata** | Ability to store & filter by document info (critical for real RAG systems)  |

### üí° Our Choice: ChromaDB

We chose **ChromaDB** for this project because:

- ‚úÖ **Zero ops** - No server setup required
- ‚úÖ **Local first** - Perfect for learning & prototyping
- ‚úÖ **Metadata support** - Filter by document properties
- ‚úÖ **Fast enough** - Handles millions of vectors
- ‚úÖ **Easy to upgrade** - Can migrate to Qdrant/Weaviate later
- ‚úÖ **Persistent storage** - No re-ingestion needed

---

## üìä Development Status & Roadmap

### Current Status

| Feature                     | Status           | Priority | Notes |
|-----------------------------|------------------|----------|-------|
| Document ingestion          | ‚úÖ Done          | High     | PDF, TXT supported |
| Recursive text chunking     | ‚úÖ Done          | High     | Structure-based |
| Semantic text chunking      | ‚úÖ Done          | High     | Meaning-based |
| Embedding generation        | ‚úÖ Done          | High     | all-MiniLM-L6-v2 |
| Vector storage (ChromaDB)   | ‚úÖ Done          | High     | Persistent HNSW |
| Similarity search           | ‚úÖ Done          | High     | Top-K retrieval |
| Automatic orchestration     | ‚úÖ Done          | High     | Smart mode detection |
| Reranking                   | üöß In Progress   | Medium   | Placeholder ready |
| LLM integration             | üìã Planned       | High     | Ollama integration |
| Prompt templates            | üìã Planned       | High     | Context formatting |
| Evaluation framework        | üìã Planned       | Medium   | Metrics & testing |
| Query optimization          | üìã Planned       | Medium   | Hybrid search |
| Web UI (Streamlit)          | üìã Planned       | Low      | User interface |
| API (FastAPI)               | üìã Planned       | Low      | REST endpoints |

**Legend**: ‚úÖ Done | üöß In Progress | üìã Planned

### Roadmap

#### Phase 1: Core Pipeline ‚úÖ (COMPLETE)
- [x] Document ingestion (PDF, TXT)
- [x] Recursive chunking
- [x] Semantic chunking
- [x] Vector embeddings (all-MiniLM-L6-v2)
- [x] ChromaDB integration
- [x] Similarity search
- [x] Automatic orchestration

#### Phase 2: LLM Integration üöß (NEXT)
- [ ] Ollama integration
- [ ] Prompt engineering
- [ ] Answer generation
- [ ] Citation tracking
- [ ] Context window management

#### Phase 3: Enhancement üìã
- [ ] Cross-encoder reranking
- [ ] Hybrid search (vector + keyword)
- [ ] Query expansion
- [ ] Multi-query retrieval
- [ ] Metadata filtering

#### Phase 4: Production üìã
- [ ] Evaluation metrics (precision, recall, F1)
- [ ] Performance monitoring
- [ ] API deployment (FastAPI)
- [ ] Streamlit web UI
- [ ] Docker containerization
- [ ] CI/CD pipeline

---

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=rag tests/

# Run specific test module
pytest tests/test_retrieval.py -v

# Run with verbose output
pytest -vv

# Generate HTML coverage report
pytest --cov=rag --cov-report=html tests/

# Run only unit tests
pytest tests/ -m unit

# Run only integration tests
pytest tests/ -m integration
```

---

## üß© Why This Architecture?

1. **Clean separation of concerns**
   - Each component has a single responsibility
   - Easy to test and debug

2. **Production-aligned structure**
   - Follows industry best practices
   - Scales from prototype to production

3. **Easy to convert into microservices**
   - Each module can become a FastAPI service
   - Ready for containerization

4. **Easy DB or model swaps**
   - Abstracted interfaces
   - Plug-and-play components

5. **Debuggable and testable**
   - Clear data flow
   - Comprehensive logging

6. **Automatic orchestration**
   - No manual mode switching
   - Intelligent system checks

---

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run linting
ruff check .

# Format code
black .

# Type checking
mypy rag/

# Run tests
pytest
```

### Code Style

- Follow PEP 8
- Use type hints
- Write docstrings (Google style)
- Keep functions focused and small
- Add tests for new features

---

## üìú License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **LangChain** - RAG framework and orchestration
- **ChromaDB** - Vector storage and similarity search
- **Sentence Transformers** - High-quality embeddings
- **Ollama** - Local LLM inference
- **HuggingFace** - Open-source models and community

---

## üìß Contact

For questions, feedback, or collaboration:

- üìß **Email**: jyotiradityaparihar@gmail.com
- üí¨ **GitHub Issues**: [Open an issue](https://github.com/your-repo/issues)
- üíº **LinkedIn**: [Jyotiraditya Singh](https://www.linkedin.com/in/jyotiraditya-singh-959488248/)

---

## üìö Additional Resources

### Learning RAG
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)
- [Building Production RAG Systems](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)

### Documentation
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [Ollama Models](https://ollama.ai/library)
- [LangChain Docs](https://python.langchain.com/)

### Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

---

## ‚ú® Status Summary

**System is stable and working up to vector retrieval.**

- ‚úÖ Ingestion pipeline: **Complete**
- ‚úÖ Vector storage: **Complete**
- ‚úÖ Query retrieval: **Complete**
- üöß LLM integration: **In Progress**
- üìã Production features: **Planned**

**Ready for LLM integration and production deployment.**

---

<div align="center">

**Built with ‚ù§Ô∏è by GRAVITY-AI for production-grade RAG systems**

‚≠ê **Star this repo** if you find it helpful !

</div>